<p style="text-align: center;"><strong><span style="font-size: large;">Representing the Group: A look at group composition's affect on representative selection and policy.</span></strong></p>
<p style="text-align: center;"><span style="font-size: small;">By Becky Neufeld and Chris Zosh</span></p>
<p style="text-align: center;">&nbsp;</p>
<p style="text-align: left;"><strong><span style="font-size: medium;">What is it?</span></strong></p>
<p style="text-align: left;"><span style="font-size: small;">We propose a model of a repeated game in which learning agents are tasked with choosing a member to represent the group. Agents vary in two ways: they have a certain policy they'd like to see implemented (represented by a bit-string), and they have one of three types of objectives from which they derive utility (altruistic, selfish, and elected). While the former represents the agent's personal interests, the latter affects their voting behavior by shaping what the agent believes the role of a leader for the group should be. In abstract, agents will vote for a member to be the leader, the leader will implement a policy, agents will get feedback on how happy the policy makes them, and then agents refine their choice for next period via a simple reinforcement learning algorithm.<br /></span></p>
<p style="text-align: left;"><span style="font-size: small;">So how does the composition of objectives agents use in the group change who is elected and what policy represents the group? First, let's dig into some model details.</span></p>
<p style="text-align: left;">&nbsp;</p>
<p style="text-align: left;"><span style="font-size: medium;"><strong>How does it work?</strong></span></p>
<p style="text-align: left;"><span style="font-size: medium;"><span style="font-size: small;">As with many simulations or repeated games, we need to establish both the initial conditions and how the game is iterated over. We provide a step by step list of the processes in our model, and proceed to dive into detail on each below.</span></span></p>
<p style="text-align: left;"><span style="text-decoration: underline;"><span style="font-size: small;">Initialization:</span></span></p>
<ol>
<li style="text-align: left;"><span style="font-size: small;">N agents are created to populate the model. Each agents is given an objective (either selfish, altruistic, or elected) a desired policy (a bit string of length v), and a initial scores for each possible action they can make. These scores will be utilized by the agent's simple learning model to guide and update decision making as the game progresses.</span></li>
</ol>
<p><span style="text-decoration: underline;"><span style="font-size: small;">Model Steps:</span></span></p>
<ol>
<li><span style="font-size: small;">First, all agents vote on a leader. They do this probabalistically by choosing proportionally to the score corresponding to each action. </span></li>
<li><span style="font-size: small;">These votes are then aggregated (via majority vote) to determine who will represent the group this period. In the case of a tie, a leader is randomly chosen from those who tied for first place.</span></li>
<li><span style="font-size: small;">The agent elected to be the representative for the group this period decides on a policy to represent the group. Implemented policies take the same form as desired ones <span style="font-size: small;">(a bit string of length v)</span>, and are <span style="font-size: small;">also decided probabalistically by choosing a value for each bit in the policy proportionally to the score corresponding to it.</span></span></li>
<li><span style="font-size: small;">Agents derive utility by evaluating the outcomes of the game this period. The benefit they derive, however, depends on the objective they were assigned at the beginning.</span></li>
<ul>
<li><span style="font-size: small;">Selfish agents care only that the representative closely represents their own interests. Mechanically, selfish agents desire to minimize the euclidean distance between their desired policy and the policy their representative implements. This distance score is normalized between 0 and 1.</span></li>
<li><span style="font-size: small;">Elected type agents have no interest or preferences over policy directly. They only desire to get elected. If elected, they derive utility equal to 1, otherwise they receive a utility of 0.</span></li>
<li><span style="font-size: small;">Altruistic agents, like selfish agents, have a desired policy they'd like to see realized. However, they beleive a representative should implement policy which takes into account everyone's needs. Mechanically, this agent calculates individual utility in the same way a selfish agent does. Their final utility for the period, however, is the weighted sum of all agent's (individual) utility.</span></li>
</ul>
<li><span style="font-size: small;">Finally, agents use their utilities from the period to reevaluate how well they think each action they took this period performed. This is done by adjusting the scores of the action taken. This is done using the following function:<br /></span></li>
</ol>
<p style="padding-left: 90px; text-align: left;"><strong><span style="font-size: small;">Score <sub>t+1</sub> (Action) = (1-&alpha;) Score&nbsp;<sub>t</sub> <strong><span style="font-size: small;">(Action) + <strong><span style="font-size: small;">&alpha; </span></strong>(Utility <sub>t </sub>+&nbsp;&szlig;*Norm_SW <sub>t</sub>) if Action taken at time t,</span></strong></span></strong></p>
<p style="padding-left: 90px; text-align: left;"><strong><span style="font-size: small;"><strong><span style="font-size: small;"><strong><span style="font-size: small;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = (1-&alpha;) Score&nbsp;<sub>t</sub> <strong><span style="font-size: small;">(Action)</span></strong></span></strong> otherwise<br /></span></strong></span></strong></p>
<p style="text-align: left; padding-left: 30px;">&nbsp;</p>
<p style="text-align: left;"><span style="font-size: medium;"><strong><strong>Results</strong></strong></span></p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;">So how does the composition of objectives agents use in the group change who is elected and what policy represents the group? Do they matter? For the purposes of this write-up, we investigate a two scenarios which have a minor change in composition. Our preliminary results seem to indicate that composition matters quite a bit.<br /></span></span></p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;">In scenario 1, we explore a model with 3 agents: 1 Selfish and 2 Altruistic. In scenario 2, we consider a model containing 1 Selfish, 1 Altruistic, and 1 Elected type agents. While, apriori, we may think this change in composition is marginal, we'll see that it changes voting and policy implementation behavior quite a bit.</span></span></p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;">In the tables below, we show the frequency of voting preferences developed by agents of each type. Table 1 corresponds to scenario 1, and Table 2 corresponds to scenario 2.<br /></span></span></p>
<p style="text-align: center;"><span style="font-size: small;"><span style="font-size: small;"><img src="fig1.png" alt="" /></span></span></p>
<p style="text-align: center;"><span style="font-size: small;"><span style="font-size: small;"><img src="fig2.png" alt="" /></span></span></p>
<p style="text-align: center;">&nbsp;</p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;">We can see that in Scenario 1, it looks like any agent can emerge as the leader, and agents are equally likely to converge on voting for a leader of either type. Since there are two altruists and one selfish agent though, it seems being selfish in this group means you're twice as likely to get elected compared to an altruist.<br /></span></span></p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;">In scenario 2, by simply switching one agent type from an altruist to elected type, a completely different voting behavior emerges. With a fairly high level of certainty, we see the elected type agent win the popular vote.</span></span></p>
<p style="text-align: left;">&nbsp;</p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;">Withing group, we also identify some behaviors which seem pretty inlign with our intiution. Altruistic agents seem more likely to implement policies further from their own desired policy (seen below).</span></span></p>
<p style="text-align: left;"><span style="font-size: small;"><span style="font-size: small;"><img style="display: block; margin-left: auto; margin-right: auto;" src="fig3.png" alt="" /></span></span><span style="font-size: medium;"><span style="font-size: small;">Curiously, we also see a difference in how certain agents are about their strategies (seen below). The plot shows with what level of certainty (with what probability) agents choose their favored action at an earlier point in the run for each scenario. The first table is for scenario 1, and the second for scenario 2. It seems the leader's problem is much easier to solve than the problem for the other two agent types.</span><br /></span></p>
<p style="text-align: center;"><span style="font-size: medium;"><span style="font-size: small;"><img src="fig4.png" alt="" /></span></span></p>
<p style="text-align: center;"><span style="font-size: medium;"><span style="font-size: small;"><img src="fig5.png" alt="" /></span></span></p>
<p style="text-align: left;"><strong><span style="font-size: medium;">Future directions</span></strong></p>
<p style="text-align: left;">&nbsp;</p>
<p style="line-height: 1.38; text-indent: 36pt; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Our future direction includes modeling directions, and exploring our model further. First we want to replicate our results by running the model for a greater number of trials and establishing convergence measures. Additionally we want to investigate if and when different specifications of the learning algorithm change the model results.</span></p>
<p style="line-height: 1.38; text-indent: 36pt; margin-top: 0pt; margin-bottom: 0pt; text-align: left;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In this write-up our results are very preliminary so we also want to invest more time in investigating the model behavior. Specifically, we want to investigate how different parameters, such as having different combinations of agent objectives, change what objectives the agents vote for, what policies are put in place and how optimal these policies are using different criteria. Specifically, we are curious about the conditions that lead agents to choose the socially versus personally optimal policies and if there are conditions where both selfish and leadership agents will choose policies that are optimal for the whole group. Along these lines we are interested in cases where selfish agents may learn to choose policy that is different from their specific goals to increase the chance they get elected as leader. It may also be the case that different combinations of agent objectives lead to selfish agents choosing altruistic agents as leaders and vice versa instead of their own objective. Overall our future directions mainly center around fully exploring the modeling behavior so more deeply answer our research questions.</span></p>
<p style="text-align: left;">&nbsp;</p>
<p style="text-align: left;">&nbsp;</p>